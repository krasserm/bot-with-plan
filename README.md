## Llama-2 agent with grammar-based sampling of function calls

Experimental implementation of a function calling interface for a Llama-2 chat model 
with [LangChain](https://github.com/langchain-ai/langchain). It uses the [grammar-based sampling](https://github.com/ggerganov/llama.cpp/pull/1773)
feature of [llama.cpp](https://github.com/ggerganov/llama.cpp) to ensure that function calls generated by the model follow 
a tool-specific schema. The resulting interface is similar to the [ChatOpenAI](https://python.langchain.com/docs/integrations/chat/openai) 
function calling interface and can be used with LangChain's [agent framework](https://python.langchain.com/docs/modules/agents/).

To increase the probability that the model generates an appropriate tool call at each step 
i.e. selects the right tool and arguments, an unconstrained reasoning phase should precede 
the constrained tool call generation phase. Otherwise, attention to the thoughts generated 
during the reasoning phase is not possible.

Llama-2 is known to have some zero-shot tool usage capabilities, but they are limited. In 
its current state, the implementation is a simple prototype for demonstrating grammar-based 
sampling in LangChain agents. It is general enough to be used with many other language models 
supported by llama.cpp, after some tweaks to the prompt templates.

More details in [example.ipynb](example.ipynb).

### Tools

To give the agent the ability to solve mathematical problems, [Llama2Math](https://github.com/krasserm/grammar-based-agents/blob/master/gba/math.py) 
implements an interface that interprets and evaluates mathematical queries using a LLM. 

See [example_math.ipynb](example_math.ipynb) for more details.

## Getting started

### Download models

```shell
mkdir models

# Download agent LLM Llama-2-70B
wget https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF/resolve/main/llama-2-70b-chat.Q4_0.gguf?download=true -O models/llama-2-70b-chat.Q4_0.gguf

# Download math LLM CodeLlama-7B
wget https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q4_K_M.gguf?download=true -O models/codellama-7b-instruct.Q4_K_M.gguf
```

### Docker image

Either build a [CUDA-enabled llama.cpp Docker image](https://github.com/ggerganov/llama.cpp/blob/master/README.md#docker-with-cuda)
yourself (`full-cuda` variant) or use the pre-build [ghcr.io/krasserm/llama.cpp:full-cuda](https://github.com/krasserm/grammar-based-agents/pkgs/container/llama.cpp)
Docker image as done in the next section.

### Launch server

#### Agent LLM

```shell
docker run --gpus all --rm -p 8080:8080 -v $(realpath models):/models ghcr.io/krasserm/llama.cpp:full-cuda \
  --server -m /models/llama-2-70b-chat.Q4_0.gguf --n-gpu-layers 83 --host 0.0.0.0 --port 8080
```

#### Math LLM

```shell
docker run --gpus all --rm -p 8088:8080 -v $(realpath models):/models ghcr.io/krasserm/llama.cpp:full-cuda \
  --server -m /models/codellama-7b-instruct.Q4_K_M.gguf --n-gpu-layers 0 --host 0.0.0.0 --port 8080
```

Depending on available GPU memory you may want to decrease the `--n-gpu-layers` argument.

### Create environment

```shell
conda env create -f environment.yml
conda activate grammar-based-agents
```

### Run examples

```shell
jupyter notebook
```

and then select `example.ipynb`.