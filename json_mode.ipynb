{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c5b327-2fd6-47a1-a3be-e75d44359a27",
   "metadata": {},
   "source": [
    "# Schema-guided generation with open LLMs\n",
    "\n",
    "OpenAI recently introduced [JSON mode](https://platform.openai.com/docs/guides/text-generation/json-mode) for its chat models. Anyscale provides a [similar service](https://www.anyscale.com/blog/anyscale-endpoints-json-mode-and-function-calling-features) that additionally supports user-defined JSON schemas. Both do not disclose how this is done but it's relatively easy to implement it with [grammar-based sampling](https://github.com/ggerganov/llama.cpp/pull/1773) in llama.cpp.\n",
    "\n",
    "For this implementation I'll use an updated version of the components introduced in [Schema-guided generation in LangChain agents](https://github.com/krasserm/grammar-based-agents/blob/wip-article-1/example_agent.ipynb). These are a LangChain [LLM proxy](gba/client/llamacpp.py) communicating with a model running on a llama.cpp server, enforcing a user-defined schema if provided, and an [LLM wrapper](gba/client/chat.py) that applies a chat prompt template to incoming messages.\n",
    "\n",
    "Schema-guided generation is demonstrated with 8-bit GGUF versions of models [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) and [Mistral-7B-instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2). Instructions for running them on a llama.cpp server are available [here](https://github.com/krasserm/grammar-based-agents/blob/master/README.md#getting-started). Application examples are taken from [this article](https://www.anyscale.com/blog/anyscale-endpoints-json-mode-and-function-calling-features).\n",
    "\n",
    "## Schema-guided generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "516e9d86-b5a0-4493-99df-a351f0ad010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "import jsonref\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from gba.client import ChatClient, Llama3Instruct, LlamaCppClient\n",
    "\n",
    "# LLM proxy for an 8-bit quantized Llama-3-8B instruct model hosted on a llama.cpp server\n",
    "llama3_llm = LlamaCppClient(url=\"http://localhost:8084/completion\", temperature=-1)\n",
    "\n",
    "# LLM wrapper that applies a Llama-3 chat prompt (+ exposes chat model interface)\n",
    "llama3_chat = Llama3Instruct(llm=llama3_llm)\n",
    "\n",
    "# Chat client used by application\n",
    "llama3_client = ChatClient(llama3_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1eec185-6c77-4a31-80c0-11172e8729a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcd80e2-e5f6-4f03-90f5-fa985e85df2f",
   "metadata": {},
   "source": [
    "### Basic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99012c6e-0482-4b16-b4e4-ec8c5fa49fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"winner_team\": \"Los Angeles Dodgers\", \"winner_score\": 4, \"loser_team\": \"Tampa Bay Rays\", \"loser_score\": 2}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GameResult(BaseModel):\n",
    "    winner_team: str\n",
    "    winner_score: int\n",
    "    loser_team: str\n",
    "    loser_score: int\n",
    "\n",
    "user_message = {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
    "\n",
    "response = llama3_client.complete(\n",
    "    messages=[system_message, user_message],\n",
    "    schema=GameResult.model_json_schema(),\n",
    ")\n",
    "response[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e943d-e4b6-40ee-be1d-d935afcd253f",
   "metadata": {},
   "source": [
    "### Handling arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d951a55f-c08f-4a53-8c79-ad3c6dcb88fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"sorted_numbers\": [2, 6, 7, 8] }'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SortResult(BaseModel):\n",
    "    \"\"\"The format of the answer.\"\"\"\n",
    "    sorted_numbers: List[int] = Field(description=\"List of the sorted numbers\")\n",
    "\n",
    "user_message = {\"role\": \"user\", \"content\": \"Sort the following numbers: 2, 8, 6, 7\"}\n",
    "\n",
    "response = llama3_client.complete(\n",
    "    messages=[system_message, user_message],\n",
    "    schema=SortResult.model_json_schema(),\n",
    ")\n",
    "response[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda573f8-ceb6-4b59-9e06-a4a29f2ae9b6",
   "metadata": {},
   "source": [
    "### Handling nested structures\n",
    "\n",
    "For handling nested structures, [jsonref](https://github.com/gazpachoking/jsonref) is used to to resolve references in schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b086c89-d84c-4cfc-add9-483860c860b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"sorted_list\": [ {\"name\": \"Carol\", \"age\": 2}, {\"name\": \"Bob\", \"age\": 7}, {\"name\": \"Alice\", \"age\": 10} ] }'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Person(BaseModel):\n",
    "    \"\"\"The object representing a person with name and age\"\"\"\n",
    "    name: str = Field(description=\"Name of the person\")\n",
    "    age: int = Field(description=\"The age of the person\")\n",
    "\n",
    "class Result(BaseModel):\n",
    "    \"\"\"The format of the answer.\"\"\"\n",
    "    sorted_list: List[Person] = Field(description=\"List of the sorted objects\")\n",
    "\n",
    "user_message = {\"role\": \"user\", \"content\": \"Alice is 10 years old, Bob is 7 and Carol is 2. Sort them by age in ascending order.\"}\n",
    "\n",
    "response = llama3_client.complete(\n",
    "    messages=[system_message, user_message],\n",
    "    schema=jsonref.replace_refs(Result.model_json_schema()),\n",
    ")\n",
    "response[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f889403-ce86-42a0-b3be-68e1b1832927",
   "metadata": {},
   "source": [
    "## System prompt extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1891be-a7ff-4cf4-8db8-99168b24f8a3",
   "metadata": {},
   "source": [
    "There is one issue though. Field descriptions in schemas are ignored because they are not included in the grammar. For example, if we add format hints to field descriptions like `... in uppercase`, they have no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24910245-dfd7-4112-9c5e-9f2c57469433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"sorted_list\": [ {\"name\": \"Carol\", \"age\": 2}, {\"name\": \"Bob\", \"age\": 7}, {\"name\": \"Alice\", \"age\": 10} ] }'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"Name of the person in uppercase\")\n",
    "    age: int = Field(description=\"The age of the person\")\n",
    "    \n",
    "class Result(BaseModel):\n",
    "    sorted_list: List[Person] = Field(description=\"List of the sorted objects\")\n",
    "\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "user_message = {\"role\": \"user\", \"content\": \"Alice is 10 years old, Bob is 7 and Carol is 2. Sort them by age in ascending order.\"}\n",
    "\n",
    "response = llama3_client.complete(\n",
    "    messages=[system_message, user_message],\n",
    "    schema=jsonref.replace_refs(Result.model_json_schema()),\n",
    ")\n",
    "response[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f02f8b-da28-4863-b4f7-b4c211909a3a",
   "metadata": {},
   "source": [
    "This can be mitigated by adding field descriptions to the system prompt. The `object_from_schema` function generates a JSON object from the provided schema with field descriptions as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "401abb05-b7e8-470e-b8c5-b75d9e500071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant. \n",
      "\n",
      "Generate JSON output in the following format:\n",
      "\n",
      "{\n",
      "  \"sorted_list\": [\n",
      "    {\n",
      "      \"name\": \"Name of the person in uppercase\",\n",
      "      \"age\": \"The age of the person\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from gba.utils import object_from_schema\n",
    "\n",
    "schema = jsonref.replace_refs(Result.model_json_schema())\n",
    "schema_instance = object_from_schema(schema)\n",
    "\n",
    "system_prompt = f\"\"\"You are a helpful assistant. \n",
    "\n",
    "Generate JSON output in the following format:\n",
    "\n",
    "{json.dumps(schema_instance, indent=2)}\"\"\"\n",
    "\n",
    "system_message = {\"role\": \"system\", \"content\": system_prompt}\n",
    "\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e49a46-874c-49c5-9e96-5ab32a5ee0a6",
   "metadata": {},
   "source": [
    "Then the output is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e771f1-2438-4049-ad5f-97aed8ef3abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"sorted_list\": [ { \"name\": \"CAROL\", \"age\": 2 }, { \"name\": \"BOB\", \"age\": 7 }, { \"name\": \"ALICE\", \"age\": 10 } ] }'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llama3_client.complete(\n",
    "    messages=[system_message, user_message],\n",
    "    schema=jsonref.replace_refs(Result.model_json_schema()),\n",
    ")\n",
    "response[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37865316-8ea3-494d-ae24-24e4abb0b7aa",
   "metadata": {},
   "source": [
    "## Support other models\n",
    "\n",
    "Using other open models is straightforward as shown here for a Mistral-7b-instruct model. You just need to replace `Llama3Instruct` with `MistralInstruct` for applying a Mistral-specific chat prompt template. Examples of other chat prompt templates are [here](https://github.com/langchain-ai/langchain/pull/8295#issuecomment-1668988543) and [here](https://github.com/langchain-ai/langchain/pull/8295#issuecomment-1811914445)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89437160-1495-47a9-9c33-d889b82ec4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"sorted_numbers\": [2, 6, 7, 8] }'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gba.client import MistralInstruct\n",
    "\n",
    "# LLM proxy for a Mistral-7b-instruct model hosted on a llama.cpp server\n",
    "mistral_llm = LlamaCppClient(url=\"http://localhost:8081/completion\", temperature=-1)\n",
    "\n",
    "# LLM wrapper that applies the Mistral chat prompt (+ exposes chat model interface)\n",
    "mistral_instruct = MistralInstruct(llm=mistral_llm)\n",
    "\n",
    "# Chat client used by application\n",
    "mistral_client = ChatClient(mistral_instruct)\n",
    "\n",
    "response = mistral_client.complete(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Sort the following numbers: 2, 8, 6, 7\"}],\n",
    "    schema=SortResult.model_json_schema(),\n",
    ")\n",
    "response[\"content\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
