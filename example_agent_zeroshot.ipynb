{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcaf6623-4997-4bec-8226-42f4f112977c",
   "metadata": {},
   "source": [
    "# From monolithic to modular open LLM agents - A system of experts via prompt chaining\n",
    "\n",
    "Goal of this article is to demonstrate how chaining simple, minimalistic zero-shot prompts can lead to useful agentic behavior even when used with smaller, general-purpose LLMs. To reliably extract specific pieces of information generated by one module and use it as input for another module, this article makes heavy use of [schema-guided generation](https://krasserm.github.io/2023/12/18/llm-json-mode/).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "LLM agents require a wide range of capabilities to function correctly. They must be able to decompose complex user instructions, plan actions, interact with their environment using tools, call tools with correct arguments, reason about observations and adjust planning if needed. \n",
    "\n",
    "Instructing a model to behave like an agent is often done with a comprehensive, more or less monolithic prompt. Commercial LLMs like GPT-4 are more capable of understanding such complex prompts and have important features like [function calling](https://platform.openai.com/docs/guides/function-calling) already built in. Open LLMs, especially smaller ones, still struggle to do so. An alternative is to fine-tune smaller open LLMs on agent behavior traces created by larger models. Common to both approaches however is that it's still a monolithic expert providing all the diverse capabilites of an agent (Figure 1, left).\n",
    "\n",
    "Inspired by [prompt chaining](https://docs.anthropic.com/claude/docs/chain-prompts) I wondered how far I can get with decomposing an LLM agent into a system of smaller, highly specialized modules, each using a simple zero-shot prompt covering only one of an agent's many capabilities (Figure 1, right). Smaller LLMs often work more reliably if prompted with a simple and narrow instruction. To further improve the performance of a module, it can optionally be fine-tuned on its specific role in the agentic system, like function calling as shown later, independent of other modules.\n",
    "\n",
    "![agents](docs/img/agent-approaches.png)\n",
    "\n",
    "Figure 1. Monolithic approach (left) vs modular approach, as used in this article (right). M: Mistral-7B-Instruct-v0.2, CL: CodeLlama-7B-instruct, NR: NexusRaven-V2-13B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5b493-121b-4433-974d-8a9d4c730e4c",
   "metadata": {},
   "source": [
    "## Modules\n",
    "\n",
    "At the core of the system is a [ReAct](https://arxiv.org/abs/2210.03629)-style agent loop that uses a planning module to plan actions. An action is defined by a selected tool and a task description. Executing the selected tool results in an observation. The agent uses short-term memory for recording task-observation pairs (scratchpad) and conversational memory for recording interactions with the user.\n",
    "\n",
    "### Planner\n",
    "\n",
    "The [planner](gba/agent.py) reasons about the user request, summarizes relevant information from previous task-observation pairs, thinks about the next useful steps (CoT), generates a task description for the very next step and selects an appropriate tool based on its name and short description (sample [prompt](docs/planner_prompt.txt) and [completion](docs/planner_completion.txt)). This information is then returned to the agent loop which executes the selected tool. The planner uses a zero-shot prompted Mistral-7B-Instruct-v0.2 model.\n",
    "\n",
    "### Summarizer\n",
    "\n",
    "Observations i.e. tool execution results can vary significantly in size and relevance. For example, a calculator may output a single number whereas a search engine may return large amounts of text, most of it not relevant for the current task. A [summarizer](gba/summary.py) uses that observation to formulate a short, task-specific answer (sample [prompt](docs/summarizer_prompt.txt) and [completion](docs/summarizer_completion.txt)). This makes it much easier for the planner to reason over past task-observation pairs. The summarizer uses a zero-shot prompted Mistral-7B-Instruct-v0.2 model.\n",
    "\n",
    "### Tools\n",
    "\n",
    "Most modules of the agent are tools and the system can be extended with further tools by implementing a common tool interface. During execution, a tool gets access to the user request, the current task description and the agent's scratchpad. It is up to the tool implementation to make use of all or only a subset of the provided information.\n",
    "\n",
    "#### Function call\n",
    "\n",
    "The function call tool wraps a user-defined function into a tool interface so that it can be selected by the planner. It binds information from the task description and previous observations to function parameters. The [default implementation](gba/tools/call/nexus.py) uses NexusRaven-V2-13B for that purpose, an LLM fine-tuned for function calling from natural language instructions (sample [prompt](docs/function_call_prompt.txt) and [completion](docs/function_call_completion.txt)). An [alternative implementation](gba/tools/call/python.py) uses a zero-shot prompted CodeLlama-7B-instruct model to generate function call arguments from instructions.\n",
    "\n",
    "#### Calculate\n",
    "\n",
    "The [calculate](gba/tools/calc.py) tool generates and executes Python code from a mathematical task description and previous (numerical) observations. The current implementation supports calculations that result in a single number (sample [prompt](docs/calculate_prompt.txt) and [completion](docs/calculate_completion.txt)). Python code running this calculation is generated with a CodeLlama-7B-instruct model. Code execution is not sandboxed; use this component at your own risk.\n",
    "\n",
    "#### Ask user\n",
    "\n",
    "This tool is used when the planner needs further input from the user. It doesn't use an LLM and simply returns the user's answer to the agent ([code](gba/tools/ask.py)).\n",
    "\n",
    "#### Respond to user\n",
    "\n",
    "A tool that generates a final answer to the original request using all previous task-observation pairs ([code](gba/tools/respond.py), sample [prompt](docs/response_prompt.txt) and [completion](docs/response_completion.txt)). This tool uses a zero-shot prompted Mistral-7B-Instruct-v0.2 model for generating the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab6261-fc70-4fb6-9488-71501add5a38",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "All models used by the modular agent are running on a llama.cpp server. Instructions for serving these LLMs are available [here](https://github.com/krasserm/grammar-based-agents/blob/master/README.md#getting-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee00b78e-aa5f-4803-9850-3e7766934763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.chat_models.llm_wrapper import Llama2Chat\n",
    "\n",
    "from gba.client import LlamaCppClient\n",
    "from gba.client.chat import MistralInstruct\n",
    "\n",
    "# Proxy for 8-bit quantized Mistral-7B-Instruct-v0.2\n",
    "mistral_instruct = MistralInstruct(\n",
    "    llm=LlamaCppClient(url=\"http://localhost:8081/completion\", temperature=-1),\n",
    ")\n",
    "\n",
    "# Proxy for 4-bit quantized CodeLlama-7B-Instruct\n",
    "code_llama = Llama2Chat(\n",
    "    llm=LlamaCppClient(url=\"http://localhost:8088/completion\", temperature=-1),\n",
    ")\n",
    "\n",
    "# Proxy for 8-bit quantized NexusRaven-V2-13B\n",
    "nexus_raven = LlamaCppClient(url=\"http://localhost:8089/completion\", temperature=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72dc61e-a4f5-47bc-973f-1c1cf4f15107",
   "metadata": {},
   "source": [
    "Custom functions used in this example are `create_event` for adding events to a calendar, `search_internet` for searching documents matching a query, and `search_images` for searching images matching a query. To keep this example simple and to avoid dependencies to external APIs, `search_internet` searches for documents in a local document store, `search_images` and `create_event` are mocked. You can replace them with other implementations or add new functions as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff545e07-41ef-4461-b271-d36eb8997cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gba.agent import Agent\n",
    "from gba.client import ChatClient\n",
    "from gba.planner import ZeroShotPlanner\n",
    "from gba.search import SearchEngine\n",
    "from gba.store import DocumentStore\n",
    "from gba.summary import ResultSummarizer\n",
    "from gba.tools import *\n",
    "\n",
    "from example_docs import DOCUMENTS\n",
    "from example_funcs import create_event\n",
    "\n",
    "store = DocumentStore(path=\".chroma\")\n",
    "engine = SearchEngine(store=store)\n",
    "\n",
    "if store.count() == 0:\n",
    "    for i, document in enumerate(DOCUMENTS):\n",
    "        store.add(identifier=str(i), document=document)\n",
    "\n",
    "summarizer = ResultSummarizer(model=mistral_instruct)\n",
    "\n",
    "tools = [\n",
    "    AskTool(),\n",
    "    CalculateTool(model=code_llama, summarizer=summarizer),\n",
    "    FunctionCallTool(model=nexus_raven, fn=create_event),\n",
    "    FunctionCallTool(model=nexus_raven, fn=engine.search_images),\n",
    "    FunctionCallTool(model=nexus_raven, fn=engine.search_internet, summarizer=summarizer),\n",
    "    RespondTool(model=mistral_instruct),\n",
    "]\n",
    "\n",
    "client = ChatClient(model=mistral_instruct)\n",
    "planner = ZeroShotPlanner(client=client, tools=tools)\n",
    "agent = Agent(planner=planner, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791d310-50ee-44ed-ba19-25aa7d050cc8",
   "metadata": {},
   "source": [
    "### Multiple tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05dfed48-9d7c-4ada-85cd-f5c4948d0fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Search the internet for information about Leonardo DiCaprio's current girlfriend.\n",
      "Call: search_internet(query='Leonardo DiCaprio\\'s current girlfriend')\n",
      "Observation: Leonardo DiCaprio's current girlfriend is Italian model Vittoria Ceretti.\n",
      "\n",
      "Task: Search the internet for information about Vittoria Ceretti's age.\n",
      "Call: search_internet(query='Vittoria Ceretti')\n",
      "Observation: Vittoria Ceretti is 25 years old.\n",
      "\n",
      "Task: Calculate the value of 25 raised to the power of 0.7.\n",
      "```python\n",
      "result = 25 ** 0.7\n",
      "```\n",
      "Observation: The value of 25 raised to the power of 0.7 is approximately 9.51827.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current girlfriend of Leonardo DiCaprio is Vittoria Ceretti, and the value of her age (which is 25) raised to the power of 0.7 is approximately 9.51827.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Who is Leonardo DiCaprio's current girlfriend and \"\n",
    "          \"what is her age raised to the power of 0.7?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e3c5892-b1e4-4e55-a327-1a72681c422e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Search the internet for the first appearance dates of octopuses and elephants on Earth.\n",
      "Call: search_internet(query='first appearance dates of octopuses and elephants on Earth')\n",
      "Observation: Octopuses first appeared around 155 million years ago during the Cretaceous period.\n",
      "\n",
      "Task: Search the internet for the first appearance date of elephants on Earth.\n",
      "Call: search_internet(query='first appearance date of elephants on Earth')\n",
      "Observation: Elephants first appeared on Earth around 55 million years ago.\n",
      "\n",
      "Task: Calculate the difference in years between the first appearance of octopuses (around 155 million years ago) and the first appearance of elephants (around 55 million years ago).\n",
      "```python\n",
      "result = 155 - 55\n",
      "```\n",
      "Observation: The difference in years between the first appearance of octopuses (around 155 million years ago) and the first appearance of elephants (around 55 million years ago) is approximately 100 million years.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Octopuses first appeared around 100 million years earlier than elephants.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"How many years did octopuses appear earlier \"\n",
    "          \"on Earth, compared to elephants?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14d0d36-bad0-4444-8855-26c57069bc9a",
   "metadata": {},
   "source": [
    "### Single tool use\n",
    "\n",
    "More precisely, use the single tool in addition to the [respond to user](#respond-to-user) tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a116634-ba28-4913-8cf3-85c8b2c2a533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Creating Martin's birthday party event on Dec. 17th, 2023 at 8pm and generating a one-line invitation.\n",
      "Call: create_event(title='Martin\\'s birthday party', date='Dec. 17th, 2023', time='8pm')\n",
      "Observation: Event Martin's birthday party successfully added to calendar, date=Dec. 17th, 2023, time=8pm\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"You're invited to Martin's birthday party on Dec. 17th, 2023 at 8pm.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Add Martin's birthday party, starting Dec. 17th 2023 8pm, \"\n",
    "          \"to my calendar and respond with a one-line invitation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad837ee-4e14-48f3-97e2-51609ed8c74f",
   "metadata": {},
   "source": [
    "### Respond directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e31eef-2a6e-4880-8596-b3a9ebf10aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't scientists trust atoms? Because they make up everything!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38132fd4-44ec-4936-95d5-852ceb34266b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, I'm here to help answer any questions you might have.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ee964-beaf-466b-ad66-6fc94966d9f4",
   "metadata": {},
   "source": [
    "### Request user feedback\n",
    "\n",
    "Asks the user to provide additional input required to complete the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "541892ee-353e-4c1f-81c4-5bf817a3c4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Could you please provide some details about your best friend, such as their name or any quirks they have?:  Michael\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: Michael\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Could you please tell me any funny or unique traits that your best friend Michael has?:  works at home\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: works at home\n",
      "\n",
      "Task: Searching the internet for a joke about someone who works from home.\n",
      "Call: search_internet(query='joke about someone who works from home')\n",
      "Observation: I'm sorry, I couldn't find a joke about someone who works from home in the provided text.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Could you please provide any additional details about your best friend Michael that might help me create a joke?:  eats chickens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: eats chickens\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Why did Michael the chicken farmer laugh while working from home? Because he was raising a good cluck of business!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Tell me a joke about my best friend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f103bae-02ab-4a6d-8198-b1c28294f121",
   "metadata": {},
   "source": [
    "### Conversational tool use\n",
    "\n",
    "Stores conversational state in a memory module separated from the agent's scratchpad. Conversational memory must be explicitly enabled with `conversational=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2089e5ed-274d-4376-9756-d37b6b0c0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_agent = Agent(planner=planner, tools=tools, conversational=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eacee1f-a175-4f33-b351-28a5d7d4122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Searching for an image of a dog...\n",
      "Call: search_images(query='dog')\n",
      "Observation: [dog_1.jpg](https://example.com/dog_1.jpg)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here is the image you requested: [dog_1.jpg](https://example.com/dog_1.jpg)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.run(\"I want an image of a dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "107df761-fc2f-4b3a-98b5-1ded89256db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Searching for an image of a brown dog.\n",
      "Call: search_images(query='brown dog')\n",
      "Observation: [brown_dog_1.jpg](https://example.com/brown_dog_1.jpg)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here is an image of a brown dog: [brown_dog_1.jpg](https://example.com/brown_dog_1.jpg)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.run(\"It should be brown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07f51697-442d-49e2-8bd9-785d9855a414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Search for an image of two brown dogs.\n",
      "Call: search_images(query='two brown dogs')\n",
      "Observation: [two_brown_dogs_1.jpg](https://example.com/two_brown_dogs_1.jpg)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here is an image of two brown dogs: [two_brown_dogs_1.jpg](https://example.com/two_brown_dogs_1.jpg)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.run(\"Find an image with two of them\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7c3d1-2b7f-49f8-b339-52b0197d6fec",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "The current implementation is very simple and minimalistic. Potential improvements may include but are not limited to:\n",
    "\n",
    "- Better decision making via with tree search like in [DFSDT](https://arxiv.org/abs/2307.16789) or [LATS](https://arxiv.org/abs/2310.04406), for example.\n",
    "- Self-improvement via [self-reflection](https://arxiv.org/abs/2303.11366) or [action learning](https://arxiv.org/abs/2402.15809), for example.\n",
    "- Function or API selection from a large database instead of enumerating them in the planner prompt.\n",
    "- Better code LLM for the calculator tool to supporting more complex instructions and result types.\n",
    "- Leverage nested and parallel function calls with [NexusRaven-V2](https://nexusflow.ai/blogs/ravenv2).\n",
    "- ...\n",
    "\n",
    "Improvements can be implemented via prompt engineering and/or fine-tuning. A later version may also implement the agent modules as Langchain-compatible chains and tools, and the agent itself as Langchain agent, but in the current experimentation phase I prefer the flexibility of a custom implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda488f-f0b0-4c88-9d1c-63cd6689f24a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Although it's almost impossible to elicit agent behavior from a 7B open LLM with a monolithic zero-shot prompt, it is possible to achieve that via prompt chaining which distributes the many responsibilities of an LLM agent across specialized modules. They are coordinated by a central planner and can be optimized individually, if needed. Schema-guided generation is an important factor to support reliable data exchange between these modules."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
