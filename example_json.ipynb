{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c5b327-2fd6-47a1-a3be-e75d44359a27",
   "metadata": {},
   "source": [
    "# Schema-guided generation with open LLMs\n",
    "\n",
    "OpenAI recently introduced [JSON mode](https://platform.openai.com/docs/guides/text-generation/json-mode) for its chat models. Anyscale provides a [similar service](https://www.anyscale.com/blog/anyscale-endpoints-json-mode-and-function-calling-features) that additionally supports user-defined JSON schemas. Both do not disclose how this is done but it's relatively easy to implement it with grammar-based sampling in llama.cpp.\n",
    "\n",
    "For this implementation I'll reuse the components introduced in [Schema-guided generation in LangChain agents](https://krasserm.github.io/2023/12/10/grammar-based-agents/). These are a LangChain [LLM proxy](https://krasserm.github.io/2023/12/10/grammar-based-agents/#llamacppclient) communicating with a model running on a llama.cpp server, enforcing a user-defined schema if provided, and an [LLM wrapper](https://krasserm.github.io/2023/12/10/grammar-based-agents/#llama2chat) that applies a chat prompt template to incoming messages.\n",
    "\n",
    "Two models are used here, a Llama-2-70b-chat model and a Mistral-7b-instruct model. Instructions for running them on a llama.cpp server are available [here](https://github.com/krasserm/grammar-based-agents/blob/wip-article-2/README.md#getting-started). Application examples are taken from [this article](https://www.anyscale.com/blog/anyscale-endpoints-json-mode-and-function-calling-features).\n",
    "\n",
    "## Schema-guided generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "516e9d86-b5a0-4493-99df-a351f0ad010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "import jsonref\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "from langchain_experimental.chat_models.llm_wrapper import Llama2Chat\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from gba.llm import LlamaCppClient\n",
    "\n",
    "# LLM proxy for a Llama-2-70b model hosted on a llama.cpp server\n",
    "llama2_llm = LlamaCppClient(url=\"http://localhost:8080/completion\", temperature=-1)\n",
    "\n",
    "# LLM wrapper that applies a Llama-2 chat prompt (+ exposes chat model interface)\n",
    "llama2_chat = Llama2Chat(llm=llama2_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1eec185-6c77-4a31-80c0-11172e8729a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessage(content=\"You are a helpful assistant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcd80e2-e5f6-4f03-90f5-fa985e85df2f",
   "metadata": {},
   "source": [
    "### Basic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99012c6e-0482-4b16-b4e4-ec8c5fa49fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"loser_score\": 2, \"loser_team\": \"Tampa Bay Rays\", \"winner_score\": 4, \"winner_team\": \"Los Angeles Dodgers\" }'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GameResult(BaseModel):\n",
    "    winner_team: str\n",
    "    loser_team: str\n",
    "    winner_score: int\n",
    "    loser_score: int\n",
    "\n",
    "human_message = HumanMessage(content=\"Who won the world series in 2020?\")\n",
    "\n",
    "response = llama2_chat.predict_messages(\n",
    "    messages=[system_message, human_message],\n",
    "    schema=GameResult.model_json_schema(),\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee46e09-727a-484e-a7f4-11bec21c8374",
   "metadata": {},
   "source": [
    "Grammar-based sampling in llama.cpp orders JSON keys alphabetically, by default. This can be customized with `prop_order`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18e018af-451d-4745-a74a-87fb02899dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"loser_team\": \"Tampa Bay Rays\", \"loser_score\": 2, \"winner_team\": \"Los Angeles Dodgers\", \"winner_score\": 4 }'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llama2_chat.predict_messages(\n",
    "    messages=[system_message, human_message],\n",
    "    schema=GameResult.model_json_schema(),\n",
    "    prop_order=[\"loser_team\", \"loser_score\", \"winner_team\", \"winner_score\"]\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e943d-e4b6-40ee-be1d-d935afcd253f",
   "metadata": {},
   "source": [
    "### Handling arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d951a55f-c08f-4a53-8c79-ad3c6dcb88fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"sorted_numbers\": [2, 6, 7, 8]}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SortResult(BaseModel):\n",
    "    \"\"\"The format of the answer.\"\"\"\n",
    "    sorted_numbers: List[int] = Field(description=\"List of the sorted numbers\")\n",
    "\n",
    "human_message = HumanMessage(content=\"Sort the following numbers: 2, 8, 6, 7\")\n",
    "\n",
    "response = llama2_chat.predict_messages(\n",
    "    messages=[system_message, human_message],\n",
    "    schema=SortResult.model_json_schema(),\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda573f8-ceb6-4b59-9e06-a4a29f2ae9b6",
   "metadata": {},
   "source": [
    "### Handling nested structures\n",
    "\n",
    "For handling nested structures, [jsonref](https://github.com/gazpachoking/jsonref) is used to to resolve references in schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b086c89-d84c-4cfc-add9-483860c860b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"sorted_list\": [ { \"age\": 2, \"name\": \"Carol\" }, { \"age\": 7, \"name\": \"Bob\" }, { \"age\": 10, \"name\": \"Alice\" } ] }'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Person(BaseModel):\n",
    "    \"\"\"The object representing a person with name and age\"\"\"\n",
    "    name: str = Field(description=\"Name of the person\")\n",
    "    age: int = Field(description=\"The age of the person\")\n",
    "\n",
    "class Result(BaseModel):\n",
    "    \"\"\"The format of the answer.\"\"\"\n",
    "    sorted_list: List[Person] = Field(description=\"List of the sorted objects\")\n",
    "\n",
    "human_message = HumanMessage(content=\"Alice is 10 years old, Bob is 7 and Carol is 2. Sort them by age in ascending order.\")\n",
    "\n",
    "response = llama2_chat.predict_messages(\n",
    "    messages=[system_message, human_message],\n",
    "    schema=jsonref.replace_refs(Result.model_json_schema()),\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f889403-ce86-42a0-b3be-68e1b1832927",
   "metadata": {},
   "source": [
    "## System prompt extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1891be-a7ff-4cf4-8db8-99168b24f8a3",
   "metadata": {},
   "source": [
    "There is one issue though. Field descriptions in schemas are ignored because they are not included in the grammar. For example, if we add format hints to field descriptions like `... in uppercase`, they have no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24910245-dfd7-4112-9c5e-9f2c57469433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"sorted_list\": [ { \"age\": 2, \"name\": \"Carol\" }, { \"age\": 7, \"name\": \"Bob\" }, { \"age\": 10, \"name\": \"Alice\" } ] }'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"Name of the person in uppercase\")\n",
    "    age: int = Field(description=\"The age of the person\")\n",
    "\n",
    "class Result(BaseModel):\n",
    "    sorted_list: List[Person] = Field(description=\"List of the sorted objects\")\n",
    "\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant.\")\n",
    "human_message = HumanMessage(content=\"Alice is 10 years old, Bob is 7 and Carol is 2. Sort them by age in ascending order.\")\n",
    "\n",
    "response = llama2_chat.predict_messages(\n",
    "    messages=[system_message, human_message],\n",
    "    schema=jsonref.replace_refs(Result.model_json_schema()),\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f02f8b-da28-4863-b4f7-b4c211909a3a",
   "metadata": {},
   "source": [
    "This can be mitigated by adding field descriptions to the system prompt. The `object_from_schema` function generates a JSON object from the provided schema with field descriptions as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "401abb05-b7e8-470e-b8c5-b75d9e500071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant. \n",
      "\n",
      "Generate JSON output in the following format:\n",
      "\n",
      "{\n",
      "  \"sorted_list\": [\n",
      "    {\n",
      "      \"name\": \"Name of the person in uppercase\",\n",
      "      \"age\": \"The age of the person\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from gba.utils import object_from_schema\n",
    "\n",
    "schema = jsonref.replace_refs(Result.model_json_schema())\n",
    "schema_instance, keys = object_from_schema(schema, return_keys=True)\n",
    "\n",
    "system_prompt = f\"\"\"You are a helpful assistant. \n",
    "\n",
    "Generate JSON output in the following format:\n",
    "\n",
    "{json.dumps(schema_instance, indent=2)}\"\"\"\n",
    "\n",
    "system_message = SystemMessage(content=system_prompt)\n",
    "print(system_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e49a46-874c-49c5-9e96-5ab32a5ee0a6",
   "metadata": {},
   "source": [
    "Then the output is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82e771f1-2438-4049-ad5f-97aed8ef3abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"sorted_list\": [ {\"age\": 2, \"name\": \"CAROL\"}, {\"age\": 7, \"name\": \"BOB\"}, {\"age\": 10, \"name\": \"ALICE\"} ] }'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llama2_chat.predict_messages(\n",
    "    messages=[system_message, human_message],\n",
    "    schema=jsonref.replace_refs(Result.model_json_schema()),\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68b310-b950-48f8-8706-ad36caded0bc",
   "metadata": {},
   "source": [
    "For generating output with JSON keys in the same order as in our schema definition, we have to set `prop_order` to the list of `keys` returned from an `object_from_schema` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec20ea96-03e0-4206-af7a-ed56fe44edd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"sorted_list\": [ {\"name\": \"CAROL\", \"age\": 2}, {\"name\": \"BOB\", \"age\": 7}, {\"name\": \"ALICE\", \"age\": 10} ] }'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llama2_chat.predict_messages(\n",
    "    messages=[system_message, human_message],\n",
    "    schema=jsonref.replace_refs(Result.model_json_schema()),\n",
    "    prop_order=keys,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37865316-8ea3-494d-ae24-24e4abb0b7aa",
   "metadata": {},
   "source": [
    "## Support other models\n",
    "\n",
    "Using other open models is straightforward as shown here for a Mistral-7b-instruct model. You just need to replace `Llama2Chat` with `MistralInstruct` for applying a Mistral-specific chat prompt template. Examples of other chat prompt templates are [here](https://github.com/langchain-ai/langchain/pull/8295#issuecomment-1668988543) and [here](https://github.com/langchain-ai/langchain/pull/8295#issuecomment-1811914445)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89437160-1495-47a9-9c33-d889b82ec4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"sorted_numbers\": [2, 6, 7, 8] }'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gba.chat import MistralInstruct\n",
    "\n",
    "# LLM proxy for a Mistral-7b-instruct model hosted on a llama.cpp server\n",
    "mistral_llm = LlamaCppClient(url=\"http://localhost:8081/completion\", temperature=-1)\n",
    "\n",
    "# LLM wrapper that applies the Mistral chat prompt (+ exposes chat model interface)\n",
    "mistral_instruct = MistralInstruct(llm=mistral_llm)\n",
    "\n",
    "response = mistral_instruct.predict_messages(\n",
    "    messages=[HumanMessage(content=\"Sort the following numbers: 2, 8, 6, 7\")],\n",
    "    schema=SortResult.model_json_schema(),\n",
    ")\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
