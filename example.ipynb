{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb3a793-de9d-4e31-8374-1d0963ece7b2",
   "metadata": {},
   "source": [
    "# Llama-2 agent with grammar-based sampling of function calls\n",
    "\n",
    "LLM agents can decompose user-defined tasks into smaller steps and use tools at each step until the task is completed. Tool usage requires reasoning about current state, deciding which tool to use next, interacting with the environment by calling that tool, making an observation, updating the current state with the observation and repeating that until done. This synergy of reasoning and acting can either be achieved via prompt engineering, fine-tuning or a combination of both. \n",
    "\n",
    "An interface to the underlying LLM should either return a tool call response (to be executed by the caller) if the LLM decides to interact with the environment or a final response to the user. An example of such an interface is OpenAI's [function calling](https://platform.openai.com/docs/guides/function-calling) interface.\n",
    "\n",
    "## Scope\n",
    "\n",
    "This article presents the results of my experiments implementing a function calling interface for a Llama-2 chat model with LangChain (full code [here](https://github.com/krasserm/grammar-based-agents)). I used the [grammar-based sampling](https://github.com/ggerganov/llama.cpp/pull/1773) feature of llama.cpp to ensure that function calls generated by the model follow a user-defined schema. The resulting interface is similar to the [ChatOpenAI](https://python.langchain.com/docs/integrations/chat/openai) function calling interface and can be used with LangChain's [agent framework](https://python.langchain.com/docs/modules/agents/).\n",
    "\n",
    "Llama-2 is known to have some zero-shot tool usage capabilities but they are limited. In its current state, the implementation is a simple prototype for demonstrating grammar-based sampling in LangChain agents. It is general enough to be used with many other language models supported by llama.cpp, after some tweaks to the prompt templates.\n",
    "\n",
    "## Agent\n",
    "\n",
    "A Llama-2 agent with grammar-based sampling of function calls can be created as follows (details in section [Components](#components)). The example uses a 4-bit quantized [Llama-2 70b chat model](https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF) running on a [llama.cpp server](https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md) (launch instructions [here](https://github.com/krasserm/grammar-based-agents/blob/master/README.md#getting-started))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c9218b-b6e2-4133-970e-ef1578840a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor  \n",
    "from langchain.tools import StructuredTool  \n",
    "from langchain_experimental.chat_models.llm_wrapper import Llama2Chat\n",
    "\n",
    "from gba.agent import Agent\n",
    "from gba.llm import LlamaCppClient  \n",
    "from gba.tool import ToolCalling\n",
    "\n",
    "from example_tools import (  \n",
    "    calculate,  \n",
    "    create_event,  \n",
    "    search_images,  \n",
    "    search_internet,  \n",
    ")\n",
    "\n",
    "# Custom LangChain LLM that interacts with a model hosted on a llama.cpp server\n",
    "llm = LlamaCppClient(url=\"http://localhost:8080/completion\", temperature=-1)  \n",
    "  \n",
    "# Converts incoming messages into a Llama-2 compatible chat prompt \n",
    "# and implements the LangChain chat model interface\n",
    "chat_model = Llama2Chat(llm=llm)  \n",
    "  \n",
    "# Layers a tool calling protocol on top of chat_model resulting in\n",
    "# an interface similar to the ChatOpenAI function calling interface\n",
    "tool_calling_model = ToolCalling(model=chat_model)  \n",
    "\n",
    "# List of tools created from Python functions and used by the agent  \n",
    "tools = [  \n",
    "    StructuredTool.from_function(search_internet),  \n",
    "    StructuredTool.from_function(search_images),  \n",
    "    StructuredTool.from_function(create_event),  \n",
    "    StructuredTool.from_function(calculate),  \n",
    "]\n",
    " \n",
    "# Custom LangChain agent implementation (similar to OpenAIFunctionsAgent)\n",
    "agent_obj = Agent.from_llm_and_tools(\n",
    "    model=tool_calling_model, \n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "# LangChain's AgentExecutor for running the agent loop\n",
    "agent = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent_obj, \n",
    "    tools=tools, \n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7e6c3d-b73f-4dd0-a7ec-add6067508c9",
   "metadata": {},
   "source": [
    "Here's an example of a request that is decomposed by the agent into multiple steps, using a tool at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c296707-dc0a-4d22-bd38-52c334e4b824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\n",
      "Reasoning: I need to call the tool 'search_internet' to find out who Leonardo DiCaprio's current girlfriend is and then calculate her age raised to the 0.24 power using the tool 'calculate'.\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `search_internet` with `{'query': \"Leonardo DiCaprio's current girlfriend\"}`\n",
      "\u001B[0m\u001B[36;1m\u001B[1;3mLeonardo di Caprio started dating Vittoria Ceretti in 2023. She was born in Italy and is 25 years old\u001B[0m\n",
      "Reasoning: I need to call another tool to obtain more information, specifically the tool \"calculate\" to calculate Vittoria Ceretti's age raised to the power of 0.24.\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `calculate` with `{'expression': '25^0.24'}`\n",
      "\u001B[0m\u001B[36;1m\u001B[1;3m2.16524\u001B[0m\n",
      "Reasoning: I have enough information to respond with a final answer to the user.\n",
      "\u001B[32;1m\u001B[1;3mVittoria Ceretti, 2.16524\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Vittoria Ceretti, 2.16524'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Who is Leonardo DiCaprio's current girlfriend and what is her age raised to the 0.24 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e1a09-d8a4-458f-acfc-17d3b62ea52a",
   "metadata": {},
   "source": [
    "The answer `Vittoria Ceretti, 2.16524` is short but correct (at the time of writing). At each step, the agent first reasons about the current state and then decides for a tool call using [grammar-based sampling on the server side](https://github.com/ggerganov/llama.cpp/pull/2532) (details in section [ToolCalling](#ToolCalling)).\n",
    "\n",
    "The agent may also decide to respond to the user directly if tool usage is not necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d934ec-7192-40fc-b1e3-e72df0cde15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\n",
      "Reasoning: I can directly respond to the user with a joke, here's one: \"Why don't scientists trust atoms? Because they make up everything!\"\n",
      "\u001B[32;1m\u001B[1;3mWhy don't scientists trust atoms? Because they make up everything!\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Why don't scientists trust atoms? Because they make up everything!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead5fea-1b79-4615-b6c7-cd59a8c4b837",
   "metadata": {},
   "source": [
    "### Conversational agent\n",
    "\n",
    "For maintaining conversational state with the user the agent can be configured with a memory object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2422924c-4fd2-47a5-b70a-6c4313fb8e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import MessagesPlaceholder  \n",
    "\n",
    "chat_history = MessagesPlaceholder(variable_name=\"chat_history\")  \n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", \n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "# Agent that additionally maintains conversational state with the user\n",
    "conversational_agent_obj = Agent.from_llm_and_tools(\n",
    "    model=tool_calling_model, \n",
    "    tools=tools, \n",
    "    extra_prompt_messages=[chat_history],\n",
    ")  \n",
    "conversational_agent = AgentExecutor.from_agent_and_tools(\n",
    "    agent=conversational_agent_obj, \n",
    "    tools=tools, \n",
    "    memory=memory, \n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f3ba25-e533-404c-a49b-a8e0b612ede8",
   "metadata": {},
   "source": [
    "Let's start the conversation by requesting an image of a brown dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab1cda4-28f5-4d54-8241-769802d3c5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\n",
      "Reasoning: I need to call the tool search_images(query: str) to find an image of a brown dog.\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `search_images` with `{'query': 'brown dog'}`\n",
      "\u001B[0m\u001B[33;1m\u001B[1;3m[brown_dog_1.jpg](https://example.com/brown_dog_1.jpg)\u001B[0m\n",
      "Reasoning: I have enough information to respond with a final answer, and I can provide the user with the URL of the image.\n",
      "\u001B[32;1m\u001B[1;3mHere is an image of a brown dog: <https://example.com/brown_dog_1.jpg>\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here is an image of a brown dog: <https://example.com/brown_dog_1.jpg>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.run(\"find an image of a brown dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4bc0bc-8d21-454a-9fe4-c882a3cf58ae",
   "metadata": {},
   "source": [
    "The next request refers to the previous one, and the agent updates the search query accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "357a9730-d06f-4ccd-bf36-a4f2e0a6c437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\n",
      "Reasoning: I need to call another tool to search for images of a running dog.\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `search_images` with `{'query': 'brown dog running'}`\n",
      "\u001B[0m\u001B[33;1m\u001B[1;3m[brown_dog_running_1.jpg](https://example.com/brown_dog_running_1.jpg)\u001B[0m\n",
      "Reasoning: I have enough information to respond with a final answer.\n",
      "\u001B[32;1m\u001B[1;3mHere is an image of a brown dog running: <https://example.com/brown_dog_running_1.jpg>\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here is an image of a brown dog running: <https://example.com/brown_dog_running_1.jpg>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.run(\"dog should be running too\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4b94fe-3bf9-4d64-b5f7-bb6ec81d5640",
   "metadata": {},
   "source": [
    "The agent was able to create the query `brown dog running` only because it had access to conversational state (`brown` is mentioned only in the first request).\n",
    "\n",
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f557e-d1ea-445c-8f0f-ad9af256420b",
   "metadata": {},
   "source": [
    "### `LlamaCppClient`\n",
    "\n",
    "`LlamaCppClient` is a proxy for a model hosted on a llama.cpp server. It implements LangChain's `LLM` interface and relies on the caller to provide a valid Llama-2 chat prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a448e4f-f42e-45b0-9251-b61c1c93999c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here's a cute image of a brown dog for you! 🐶💩\\n\\n[Image description: A brown dog with a wagging tail, sitting on a green grassy field. The dog has a friendly expression and is looking directly at the camera.]\\n\\nI hope this image brings a smile to your face! Is there anything else I can assist you with? 😊\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"<s>[INST] <<SYS>>\\nYou are a helpful assistant\\n<</SYS>>\\n\\nFind an image of brown dog [/INST]\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddc07ba-97cb-458d-8d03-fba119da6f93",
   "metadata": {},
   "source": [
    "If a tool schema is provided, it is converted by `LlamaCppClient` to a grammar used for constrained sampling on the server side so that the output is a valid instance of that schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9124bed-dcaa-44f3-96e9-cc01f6256408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'object',\n",
       " 'properties': {'tool': {'const': 'search_images'},\n",
       "  'arguments': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string'}}}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gba.tool import tool_to_schema\n",
    "\n",
    "tool_schema = tool_to_schema(StructuredTool.from_function(search_images))\n",
    "tool_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20be5559-0e09-4666-a0f1-dad033086ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"tool\": \"search_images\", \"arguments\": { \"query\": \"brown dog\" } }'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt, schema=tool_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae585f-1137-4ee0-a8ef-85de4c5237d3",
   "metadata": {},
   "source": [
    "### `Llama2Chat`\n",
    "\n",
    "[Llama2Chat](https://python.langchain.com/docs/integrations/chat/llama2_chat) wraps `llm` and implements a chat model interface that applies the Llama-2 chat prompt to incoming messages (see also [these](https://github.com/langchain-ai/langchain/pull/8295#issuecomment-1668988543) [examples](https://github.com/langchain-ai/langchain/pull/8295#issuecomment-1811914445) for other chat prompt formats). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e360b6ed-a260-4a4b-a339-e3c9f808733c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, here's a cute image of a brown dog for you! 🐶💩\\n\\n[Image description: A brown dog with a wagging tail, sitting on a green grassy field. The dog has a friendly expression and is looking directly at the camera.]\\n\\nI hope this image brings a smile to your face! Is there anything else I can assist you with? 😊\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant\"),\n",
    "    HumanMessage(content=\"Find an image of brown dog\"),\n",
    "]\n",
    "chat_model.predict_messages(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61baa00e-6e67-40d3-a015-b9b83c130f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{ \"tool\": \"search_images\", \"arguments\": { \"query\": \"brown dog\" } }')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.predict_messages(messages=messages, schema=tool_to_schema(StructuredTool.from_function(search_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6214fe7f-381d-4f49-8b5a-6f4dd57a8952",
   "metadata": {},
   "source": [
    "### `ToolCalling`\n",
    "\n",
    "`ToolCalling` adds tool calling functionality to the wrapped `chat_model`, resulting in an interface very similar to the function calling interface of `ChatOpenAI`. It converts a list of provided tools to a [oneOf](https://json-schema.org/understanding-json-schema/reference/combining#oneof) JSON schema and submits it to the wrapped chat model. It also constructs a system prompt internally that informs the backend LLM about the presence of these tools and adds a special `respond_to_user` tool used by the LLM for providing a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11bd32fd-3e89-4aa8-bf0c-d8e2804071c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reasoning: I need to call the calculate tool to evaluate the expression and obtain the result.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_call': {'tool': 'calculate', 'arguments': {'expression': '(2 * 5) ** 0.8 / 2'}}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [HumanMessage(content=\"What is (2 * 5) raised to power of 0.8 divided by 2?\")]\n",
    "tool_calling_model.predict_messages(messages=chat, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759afbaf-6781-45ac-85c8-e4f4b2237078",
   "metadata": {},
   "source": [
    "The wrapped `chat_model` first receives a message with an instruction to reason about the current state. This message doesn't contain a schema so that the model output is unconstrained. Then the model receives another message with an instruction to act i.e. respond with a tool call. This message contains the JSON schema of the provided tools so that the tool call can be generated with grammar-based sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b30041-e10d-4acf-bf9c-a83cfd84fa0c",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17faa46-9da6-4819-962f-bfe347fa270e",
   "metadata": {},
   "source": [
    "Forcing a model to generate tool calls with grammar-based sampling ensures that they follow a specific schema. To increase the probability that the model generates an appropriate tool call at each step i.e. selecting the right tool and arguments, an unconstrained reasoning phase should precede the tool call generation phase. Otherwise attention to the thoughts generated during the reasoning phase is not possible. This two-step approach may also be the basis for using more specialized models e.g. one for the reasoning phase and another one for the tool call generation phase. I plan to implement extensions with more specialized open-source LLMs later and add them to the [GitHub repo](https://github.com/krasserm/grammar-based-agents)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
